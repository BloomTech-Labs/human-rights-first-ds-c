{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_explore_news3k.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3DoGf2g-isd",
        "outputId": "f9af8063-cf2c-4fd6-fd9b-ce9343c6be3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# install PRAW and newspaper3k\n",
        "!pip install praw\n",
        "!pip3 install newspaper3k"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.6/dist-packages (7.1.0)\n",
            "Requirement already satisfied: prawcore<2.0,>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from praw) (1.5.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.6/dist-packages (from praw) (0.57.0)\n",
            "Requirement already satisfied: update-checker>=0.17 in /usr/local/lib/python3.6/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from prawcore<2.0,>=1.3.0->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (1.24.3)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (6.0.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.6/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC5g2McBACIs"
      },
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import praw\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pickle\n",
        "from newspaper import Article\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFBhzgU0AGwA"
      },
      "source": [
        "def lowerify(text):\n",
        "    # fix up geolocation dataframe a little\n",
        "    #make all lowercase\n",
        "    return text.lower()\n",
        "\n",
        "# set up cities/states locations datafrane\n",
        "locs_path = 'https://raw.githubusercontent.com/Lambda-School-Labs/Labs25-Human_Rights_First-TeamB-DS/main/project/cities_states.csv'\n",
        "locs_df = pd.read_csv(locs_path)\n",
        "locs_df = locs_df.drop(columns=['Unnamed: 0', 'country'])\n",
        "locs_df['city_ascii'] = locs_df['city_ascii'].apply(lowerify)\n",
        "locs_df['admin_name'] = locs_df['admin_name'].apply(lowerify)\n",
        "\n",
        "# state to city lookup table\n",
        "# mapping output values list \n",
        "states_map = {}\n",
        "for state in list(locs_df.admin_name.unique()):\n",
        "    states_map[state] = locs_df[locs_df['admin_name'] == state]['city_ascii'].to_list()\n",
        "\n",
        "# police brutality indentifying nlp\n",
        "# make sure to import model.pkl\n",
        "model_file = open('model.pkl', 'rb')\n",
        "pipeline = pickle.load(model_file)\n",
        "model_file.close()\n",
        "\n",
        "# spacy nlp model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Set up PRAW\n",
        "# PRAW credentials go here\n",
        "reddit = praw.Reddit(client_id='97nXmEBrjK3CIw', client_secret=\"WIE6O_ECbv6zaSR3m8au8fWrnwc\", password=\"Gwill123\", user_agent='hrf-labs27', username='griffinwilson')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XasAAbgrIMg2",
        "outputId": "37d6e5cc-8750-445f-f2db-8044416bd419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Grab data from reddit\n",
        "data = []\n",
        "print('Pulling data from Reddit...')\n",
        "for submission in reddit.subreddit(\"news\").top('week', limit=500):\n",
        "    data.append([\n",
        "        submission.id, submission.title, submission.url\n",
        "    ])\n",
        "# construct a dataframe with the data\n",
        "col_names = ['id', 'title', 'url']\n",
        "df = pd.DataFrame(data, columns=col_names)\n",
        "print(f'Number of entries initially pulled: {df.shape[0]}\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pulling data from Reddit...\n",
            "Number of entries initially pulled: 428\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sotATzlmIQ-K",
        "outputId": "ee5158b3-7fec-4de1-a10e-3eb950228862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# pull the text from each article itself using newspaper3k\n",
        "content_list = []\n",
        "date_list = []\n",
        "# go through each URL and use newspaper3k to extract data\n",
        "print('Extracting data via newspaper3k...')\n",
        "for id_url in df['url']:\n",
        "    # use newspaper3k to extract text\n",
        "    article = Article(id_url)\n",
        "    article.download()\n",
        "    # if the article doesn't download, the error is thrown in parse()\n",
        "    try:\n",
        "        article.parse()\n",
        "    except:\n",
        "        # add null values to show no connection\n",
        "        content_list.append(None)\n",
        "        date_list.append(None)\n",
        "        continue\n",
        "    content_list.append(article.text)\n",
        "    # this will be null if newspaper3k can't find it\n",
        "    date_list.append(article.publish_date)\n",
        "df['text'] = content_list\n",
        "df['date'] = date_list\n",
        "print('Number of entries with missing data:')\n",
        "print(df.isnull().sum(),'\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data via newspaper3k...\n",
            "Number of entries with missing data:\n",
            "id         0\n",
            "title      0\n",
            "url        0\n",
            "text      17\n",
            "date     151\n",
            "dtype: int64 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES2Q7ZWxIhhj",
        "outputId": "245d3676-382a-42d3-d813-613623f5c7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# drop any articles with missing data columns\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns='index')\n",
        "print(f'Resulting entry count: {df.shape[0]}\\n')\n",
        "\n",
        "# convert date column to pandas Timestamps\n",
        "def timestampify(date):\n",
        "    return pd.Timestamp(date, unit='s').isoformat()\n",
        "df['date'] = df['date'].apply(timestampify)\n",
        "\n",
        "print('Filtering through police brutality filter...')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resulting entry count: 277\n",
            "\n",
            "Filtering through police brutality filter...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jpYA0I7IoZx",
        "outputId": "4be5aa73-a1a5-48e7-8bca-ad101c826e4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use NLP model to filter posts\n",
        "df['is_police_brutality'] = pipeline.predict(df['title'])\n",
        "df = df[df['is_police_brutality'] == 1]\n",
        "df = df.drop(columns='is_police_brutality')\n",
        "print(f'Number of entries determined to be about police brutality: {df.shape[0]}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of entries determined to be about police brutality: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF-l2WQ8IqOq",
        "outputId": "8b856863-9563-4136-c0c6-4861d6444c3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use spaCy to extract location tokens\n",
        "tokens_list = []\n",
        "print('Tokenizing through spaCy...')\n",
        "for text in df['text']:\n",
        "    doc = nlp(text)\n",
        "    ents = [e.text.lower() for e in doc.ents if e.label_ == 'GPE']\n",
        "    tokens_list.append(ents)\n",
        "df['tokens'] = tokens_list"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing through spaCy...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c1yndWC08z",
        "outputId": "19e1cc91-1ff4-4903-a3c5-3d36a93b4e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# figure out which city and state the article takes place in\n",
        "city_list = []\n",
        "state_list = []\n",
        "geo_list = []\n",
        "print('Compiling geolocation data...')\n",
        "for tokens in df['tokens']:\n",
        "    # set up Counter\n",
        "    c = Counter(tokens)\n",
        "\n",
        "    # set up geolocation dict for geo list\n",
        "    geo_entry = {'lat': None, 'long': None}\n",
        "\n",
        "    # count which states come back the most, if any\n",
        "    state_counts = {}\n",
        "    for state in states_map:\n",
        "        if c[state] > 0:\n",
        "            state_counts[state] = c[state]\n",
        "\n",
        "    # get state(s) that came back the most as dict with lists\n",
        "    max_count = 0\n",
        "    max_state = None\n",
        "\n",
        "    for state in state_counts:\n",
        "        if state_counts[state] > max_count:\n",
        "            max_count = state_counts[state]\n",
        "            max_state = {state: {}}\n",
        "        elif state_counts[state] == max_count:\n",
        "            max_state[state] = {}\n",
        "\n",
        "    # if no state is found\n",
        "    if max_state is None:\n",
        "        city_list.append(None)\n",
        "        state_list.append(None)\n",
        "        geo_list.append(geo_entry)\n",
        "        continue\n",
        "\n",
        "    max_city = None\n",
        "    # get any cities in tokens based on states\n",
        "    for state in max_state:  # ideally this should only run once\n",
        "        city_counts = {}\n",
        "        for city in states_map[state]:\n",
        "            if c[city] > 0:\n",
        "                city_counts[city] = c[city]\n",
        "        max_state[state] = city_counts\n",
        "\n",
        "        # get the city/state combo that came back the most\n",
        "        max_count = 0\n",
        "        for city in city_counts:\n",
        "            if city_counts[city] > max_count:\n",
        "                max_count = city_counts[city]\n",
        "                max_city = (city, state)\n",
        "\n",
        "        # if no city is found\n",
        "    if max_city is None:\n",
        "        city_list.append(None)\n",
        "        state_list.append(None)\n",
        "        geo_list.append(geo_entry)\n",
        "        continue\n",
        "\n",
        "    # the city and state should be known now\n",
        "\n",
        "    city_list.append(max_city[0].title())\n",
        "    state_list.append(max_city[1].title())\n",
        "    # now get the geolocation data\n",
        "    row = locs_df[(\n",
        "        (locs_df['city_ascii'] == max_city[0]) &\n",
        "        (locs_df['admin_name'] == max_city[1])\n",
        "    )]\n",
        "    row = row.reset_index()\n",
        "    if row.empty:\n",
        "        pass\n",
        "    else:\n",
        "        geo_entry['lat'] = row['lat'][0]\n",
        "        geo_entry['long'] = row['lng'][0]\n",
        "    geo_list.append(geo_entry)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compiling geolocation data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUl01CUvI5yh",
        "outputId": "ae131869-45dd-4c6d-9d22-0793e516819e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "source": [
        "#loop ends, add cities and states onto dataframe\n",
        "df['city'] = city_list\n",
        "df['state'] = state_list\n",
        "df['geocoding'] = geo_list\n",
        "print('Number of entries where geolocation data could not be found:')\n",
        "print(df.isnull().sum(),'\\n')\n",
        "\n",
        "# drop any columns with null entries for location\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "df = df.drop(columns='index')\n",
        "\n",
        "# cleanup to match 846 api\n",
        "def listify(text):\n",
        "    return [text]\n",
        "df['links'] = df['url'].apply(listify)\n",
        "df['description'] = df['text']\n",
        "df = df.drop(columns=['tokens', 'text'])\n",
        "df = df[[\n",
        "    'id', 'state', 'city',\n",
        "    'date', 'title', 'description',\n",
        "    'links', 'geocoding'\n",
        "]]\n",
        "\n",
        "print(f'Final number of entries: {df.shape[0]}')\n",
        "df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of entries where geolocation data could not be found:\n",
            "id            0\n",
            "title         0\n",
            "url           0\n",
            "text          0\n",
            "date          0\n",
            "tokens        0\n",
            "city         13\n",
            "state        13\n",
            "geocoding     0\n",
            "dtype: int64 \n",
            "\n",
            "Final number of entries: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>state</th>\n",
              "      <th>city</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>links</th>\n",
              "      <th>geocoding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>j3d4ia</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>Portland</td>\n",
              "      <td>2020-10-01T00:00:00</td>\n",
              "      <td>US Justice Department cracks down on Portland ...</td>\n",
              "      <td>At the end of August, in a green leafy park in...</td>\n",
              "      <td>[https://www.opb.org/article/2020/10/01/us-jus...</td>\n",
              "      <td>{'lat': 45.5371, 'long': -122.65}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>j2c6rw</td>\n",
              "      <td>California</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>2020-09-30T00:42:00+00:00</td>\n",
              "      <td>San Jose officer facing assault charge in viol...</td>\n",
              "      <td>A San Jose, California, police officer is faci...</td>\n",
              "      <td>[https://www.nbcnews.com/news/us-news/san-jose...</td>\n",
              "      <td>{'lat': 37.3021, 'long': -121.8489}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>j2qr2c</td>\n",
              "      <td>California</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>2020-09-30T13:19:54+00:00</td>\n",
              "      <td>San Jose Officer Facing Assault Charge in Viol...</td>\n",
              "      <td>A San Jose, California, police officer is faci...</td>\n",
              "      <td>[https://www.nbcboston.com/news/national-inter...</td>\n",
              "      <td>{'lat': 37.3021, 'long': -121.8489}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>j4e5ys</td>\n",
              "      <td>California</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>2020-10-03T05:19:51+00:00</td>\n",
              "      <td>Better weather won't keep California from grim...</td>\n",
              "      <td>SAN FRANCISCO (AP) â€” Red flag warnings of extr...</td>\n",
              "      <td>[https://apnews.com/article/wildfires-fires-ca...</td>\n",
              "      <td>{'lat': 37.7562, 'long': -122.443}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ...                            geocoding\n",
              "0  j3d4ia  ...    {'lat': 45.5371, 'long': -122.65}\n",
              "1  j2c6rw  ...  {'lat': 37.3021, 'long': -121.8489}\n",
              "2  j2qr2c  ...  {'lat': 37.3021, 'long': -121.8489}\n",
              "3  j4e5ys  ...   {'lat': 37.7562, 'long': -122.443}\n",
              "\n",
              "[4 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}